[Q1]
After several tries :
Loss percentage observed : 0%
Goodput : 100.2 kbps
Except for the first second, the goodput is less.

[Q2]
We observe a goodput at around 9160.6kbps

[Q3]
1000 bytes of data + 8 bytes UDP header + 20 bytes IP header = 1028 bytes of data in the ethernet frame
ethernet frame header : 14 bytes minimum therefore 1042 bytes total for the ethernet frame
This is exactly what we observe on wireshark.

[Q4]
We call throuhput the rate of sent bytes (app, udp, ip and ethernet).
As we saw, for 1000 bytes of application data we "lose" 44 bytes for headers.
Therefore the goodput is 1000/1042 the throughput.

The max goodput x and bottleneck limit y follows the relation : 
y * 1000/1042 = x

Therefore, if the max throughput is y = 5Mbps, the max goodput is x ~= 4,798Mbps
That is 4798 kbps.


[Q5]
The loss percentage is 0%
The goodput is (average) 100.1kbps

[Q6]
- 5Mbps
Loss : 4.141 %
Goodput : 4798.6 kbps

Here we observe that we just pass the theoritical rate limit of the router, that is, we achieve the theoritical goodput while having a bit of loss due to the fact that we are sending strictly more application than the bottleneck rate can permit (5Mbps vs 4.78 Mbps).
The goodput is slightly more than the theoritical's, it may be due to approximation in time or data received.

- 10Mbps
Loss : 52.033%
Goodput : 4800.8

Here we observe the loss increasing greatly. In fact we are trying to send at more than 2 times the theoritical limit.
The goodput is approximately the same as with 5Mbps, which is approximately the same as the theoritical goodput.
This is because the router drops roughly one packet for 2 received, and process the rest, since there is only one flow this explains the results.

[Q7]
1000 bytes of data + 32 bytes TCP header + 20 bytes IP header = 1052 bytes of data in the ethernet frame (with no TCP options)
ethernet frame header : 14 bytes minimum therefore 1066 bytes total for the ethernet frame (minimum)
On Wireshark we observe 1066 bytes.

[Q8]
See Q4 for the same explanations
The max goodput x and bottleneck limit y follows the relation : 
y * 1000/1052 = x

For y = 5Mbps, the goodput is x = 4,754Mbps.

[Q9]
After some time, the goodput is 4662.4 kbps.
We see that it is less than the theoritical, which is normal, but not so close.
This is because of TCP congestion control (here cubic), we lose some goodput because of the multiplicative decrease.

[Q16]
The observed RTT remains around 300ms, never going under that value. This is expected because of the delay we just added to h3 on its eth0 interface.

[Q17]
Let x1 and x2 be the rates allocated by TCP to a single flow for h1 and h2 respectively.
Assuming Proportional Fairness, we must maximize the utility function U(x) = log(x):

max U = log(x1) + 3 * log(x2)

where x1 + x2 < c, since all four flows share the link r1-h3. Setting x1 = x, we have x2 = (c - x) / 3. We obtain the following function to maximize:

f(x) = log(x) + 3 * log((c - x) / 3)

f'(x) = (1 / x) - (3 / (c - x))

After solving f'(x) = 0, we find the solution x = (c / 4). With c = 5 Mbps, this results in the following allocation:

x1 = 5 / 4 = 1.25 Mbps                   => h1 goodput = 1.25 Mbps
x2 = (5 - x1) / 3 = 3.75 / 3 = 1.25 Mbps => h2 goodput = 1.25 * 3 = 3.75 Mbps

All flows get the same rate. This means that overall h2 gets three times the goodput of h1.

[Q18]
After the values stabilize more or less, we have the following results:
- The average goodput at h1 is around 1200kbps
- The average goodput at each of h2's clients is around 1100kbps, so the average aggregate goodput at h2 is around 3300kbps
This corresponds more or less to our theoretical analysis. Apart from standard fluctuation/stabilization issues, the slight differences could be due to the fact that the version of TCP being used is CUBIC which, as we saw in class, has a slightly different utility function than Proportional Fairness, which we assumed in Q17.

[Q19]
The RTTs observed on the four clients are almost always significantly higher than 300ms, reaching up to 350ms. This increase is significant, especially in a virtual environment like this, which means that there is most probably a queuing delay.

[Q33]
Let n1 be the first node connecting A and C to the common link,
Let n2 be the node connecting B and D to the common link.
At n1, the ratio of accepted traffic is 2.5/20+2 = 0.1136
Therefore on the common link, A's flow is at max 0.2272=2 * 0.1136 Mbps and C's
flow at max 2.272 Mbps.
At n2, A's flow can go to B without rate reduction, while C's flow has to be red
uced to at max 0.5 Mbps.

Finally, the max goodput at B is 0.2272 Mbps and the max goodput at D is 0.5 Mbp
s.

[Q34]
Applying the water filling algorithm, we first set xA and xC to 0.5Mbps, and the
n we can improve xA to be 2 Mbps.
Then no improvments can be made because it saturates the common link.
Therefore a max-min allocation is 2Mbps for A, 0.5Mbps for C.

[Q35]
We have this set of constraints for A's rate xA and C's rate xC :
xA + xC <= 2.5
xC <= min(0.5, 20) = 0.5
xA <= min(2, 20) = 2
But xA, xC = 2, 0.5 satisfies equality on theses constraints, therefore is a proportional allocation.

[Q36]
- h1 <-> h4
goodput :56.4 (varies a lot from 2kbps to 480kbps)
loss : 93.331%
- h2 <-> h3
goodput : 480 kbps
loss: 97.614%

[Q37]
With rates 2Mbps for h1 and 0.5Mbps for h2, we get 
- h1 <-> h4
goodput : 1911.1 kbps
loss : 4.516%
- h2 <-> h3
goodput : 479.9 kbps
loss: 3.460%

We clearly see an improvements in the loss rate, even though the rates are lower than in q36 : congestion collapse.

[Q38] 
- h1 <-> h4 goodput : 1849.1kbps
- h2 <-> h3 goodput : 467.4kbps
Which is close to the max min allocation (a bit worse).

[Q39]
We saw that congestion collapse occures when the sending rates are decided independently of the network topology and with no congestion control.
With knowledge of the network topology and bottlenecks, one can set the rates so that no congestion control is necessary.
However a stable network topology is rare, therefore the best way to ensure no congestion collapse and use the links fairly and efficiently is to have an adaptative congestion control mechanism, which have advantages :
- automatic, no knowledge required about the current network states.
- adaptative, adapts itself to the current network state
- avoids congestion
- divide "fairly" the network ressources between flows
- allows a good efficiency for links and good usage
