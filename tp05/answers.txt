[Q1]
After several tries :
Loss percentage observed : 0%
Goodput : 100.2 kbps
Except for the first second, the goodput is less.

[Q2]
We observe a goodput at around 9160.6kbps

[Q3]
1000 bytes of data + 8 bytes UDP header + 20 bytes IP header = 1028 bytes of data in the ethernet frame
ethernet frame header : 14 bytes minimum therefore 1042 bytes total for the ethernet frame
This is exactly what we observe on wireshark.

[Q4]
We call throuhput the rate of sent bytes (app, udp, ip and ethernet).
As we saw, for 1000 bytes of application data we "lose" 44 bytes for headers.
Therefore the goodput is 1000/1042 the throughput.

The max goodput x and bottleneck limit y follows the relation : 
y * 1000/1042 = x

Therefore, if the max throughput is y = 5Mbps, the max goodput is x ~= 4,798Mbps
That is 4798 kbps.


[Q5]
The loss percentage is 0%
The goodput is (average) 100.1kbps

[Q6]
- 5Mbps
Loss : 4.141 %
Goodput : 4798.6 kbps

Here we observe that we just pass the theoritical rate limit of the router, that is, we achieve the theoritical goodput while having a bit of loss due to the fact that we are sending strictly more application than the bottleneck rate can permit (5Mbps vs 4.78 Mbps).
The goodput is slightly more than the theoritical's, it may be due to approximation in time or data received.

- 10Mbps
Loss : 52.033%
Goodput : 4800.8

Here we observe the loss increasing greatly. In fact we are trying to send at more than 2 times the theoritical limit.
The goodput is approximately the same as with 5Mbps, which is approximately the same as the theoritical goodput.
This is because the router drops roughly one packet for 2 received, and process the rest, since there is only one flow this explains the results.

[Q7]
1000 bytes of data + 32 bytes TCP header + 20 bytes IP header = 1052 bytes of data in the ethernet frame (with no TCP options)
ethernet frame header : 14 bytes minimum therefore 1066 bytes total for the ethernet frame (minimum)
On Wireshark we observe 1066 bytes.

[Q8]
See Q4 for the same explanations
The max goodput x and bottleneck limit y follows the relation : 
y * 1000/1052 = x

For y = 5Mbps, the goodput is x = 4,754Mbps.

[Q9]
After some time, the goodput is 4662.4 kbps.
We see that it is less than the theoritical, which is normal, but not so close.
This is because of TCP congestion control (here cubic), we lose some goodput because of the multiplicative decrease.

[Q16]
The observed RTT remains around 300ms, never going under that value. This is expected because of the delay we just added to h3 on its eth0 interface.

[Q17]
Let x1 and x2 be the rates allocated by TCP to a single flow for h1 and h2 respectively.
Assuming Proportional Fairness, we must maximize the utility function U(x) = log(x):

max U = log(x1) + 3 * log(x2)

where x1 + x2 < c, since all four flows share the link r1-h3. Setting x1 = x, we have x2 = (c - x) / 3. We obtain the following function to maximize:

f(x) = log(x) + 3 * log((c - x) / 3)

f'(x) = (1 / x) - (3 / (c - x))

After solving f'(x) = 0, we find the solution x = (c / 4). With c = 5 Mbps, this results in the following allocation:

x1 = 5 / 4 = 1.25 Mbps                   => h1 goodput = 1.25 Mbps
x2 = (5 - x1) / 3 = 3.75 / 3 = 1.25 Mbps => h2 goodput = 1.25 * 3 = 3.75 Mbps

All flows get the same rate. This means that overall h2 gets three times the goodput of h1.

[Q18]
After the values stabilize more or less, we have the following results:
- The average goodput at h1 is around 1200kbps
- The average goodput at each of h2's clients is around 1100kbps, so the average aggregate goodput at h2 is around 3300kbps
This corresponds more or less to our theoretical analysis. Apart from standard fluctuation/stabilization issues, the slight differences could be due to the fact that the version of TCP being used is CUBIC which, as we saw in class, has a slightly different utility function than Proportional Fairness, which we assumed in Q17.

[Q19]
The RTTs observed on the four clients are almost always significantly higher than 300ms, reaching up to 350ms. This increase is significant, especially in a virtual environment like this, which means that there is most probably a queuing delay.

[Q20]
We apply a similar reasoning to what we've done in Q17. We need to maximize the utility function of TCP RENO which is U(x) = (sqrt(2) / RTT) * arctan(x * RTT / sqrt(2)):

max U = (sqrt(2) / RTT1) * arctan(x1 * RTT1 / sqrt(2)) + 3 * (sqrt(2) / RTT2) * arctan(x2 * RTT2 / sqrt(2))

We set x1 = x and x2 = (c - x) / 3. We then derive the obtained function f (the math is long for this one so we didn't include it), and set f'(x) = 0, which results in a quadratic equation that has two solutions:

(1) x = (c * RTT2 * (RTT2 - 3 * RTT1)) / (RTT2^2 - 9 * RTT1^2)
(2) x = (c * RTT2 * (RTT2 + 3 * RTT1)) / (RTT2^2 - 9 * RTT1^2)

We can now plug in c = 5, RTT1 = 0.3 + Y, RTT2 = 0.3 (we use here 0.3 instead of 300 to keep all time units in seconds, as c is in Mbps). After simplification, we obtain:

(1) x =   (0.9 + 4.5 * Y) / (0.72 + 5.4 * Y + 9 * Y^2)
(2) x = - (  6 + 4.5 * Y) / (0.72 + 5.4 * Y + 9 * Y^2)

Since Y > 0, we can immediately rule out (2) which would give a negative allocation to x1. This means that the goodput that h1 and h2 will get as a function of Y is given by the following:

h1 -> x1     =       (0.9 + 4.5 * Y) / (0.72 + 5.4 * Y + 9 * Y^2)
h2 -> 3 * x2 = (c - ((0.9 + 4.5 * Y) / (0.72 + 5.4 * Y + 9 * Y^2)))

Numerically, we just need to replace Y by 0.2 and 0.4 for scenarios D2 and D3 respectively:

[Q20a/(i) ] ~0.83
[Q20a/(ii)] ~4.17
[Q20b/(i) ] ~0.63
[Q20b/(ii)] ~4.38

[Q21]
Here are the measured aggregate goodputs for h1 and h2:

[Q21a/(i) ] D2: h1 -> ~2070 kbps
[Q21a/(ii)] D2: h2 -> ~2390 kbps
[Q21b/(i) ] D3: h1 -> ~2080 kbps
[Q21b/(ii)] D3: h2 -> ~2335 kbps

[Q22]
We now switch to TCP RENO. Here are the measured aggregate goodputs for h1 and h2:

[Q22a/(i) ] D2: h1 -> ~1590 kbps
[Q22a/(ii)] D2: h2 -> ~2400 kbps
[Q22b/(i) ] D3: h1 -> ~1250 kbps
[Q22b/(ii)] D3: h2 -> ~2660 kbps

[Q23]
The results for TCP RENO do not correspond very well to our theoretical analysis.

[Q24]
As we saw in class, TCP CUBIC is independent of the RTT measured for a given host. This explains why the measured goodputs are very close in scenarios D2 and D3 each with a different RTT at h1. On the other hand, this is not the case for TCP RENO whose rate allocations depends highly on the observed RTT. This is why with TCP RENO, we see that h1's average goodput is reduced by around 300ms between scenarios D2 and D3.
TCP CUBIC works better in a regime where the RTT or product delay-bandwidth values are considered to be high. This is mainly because in a network with such properties, the limit that congestion control would impose on the congestion window can be so high that it is simply inefficient to do additive increase as it would take a lot of time to reach the limit once again after each multiplicative decrease. With cubic increase, TCP CUBIC tries to get closer to the limit a little bit more quickly and therefore make better use of the resources. That along with the 0.7 multiplicative decrease (compared to 0.5 with RENO) basically aims to attenuate the "sawtooth" effect of TCP for such networks.

[Q25]
It is relatively easy to find a max-min fair allocation by applying the water-filling algorithm. In this case, we keep increasing the allocations for all three flows, and both bottlenecks with 2Mbps bandwidth will become saturated when all three flows are allocated an equal rate of 1Mbps. At this point, all three sources are frozen, the algorithm terminates and we have our allocations.

[Q26]
For proportional fairness, we need to maximize the utility function just like we did earlier:

max U = log(x1) + log(x2) + log(x3)

We can set x = x1, x2 = 2 - x and x3 = 2 - x:

f(x) = log(x) + 2 * log(2 - x)

f'(x) = (1 / x) - (2 / (2 - x))

Solving f'(x) = 0, we find the solution x = 2 / 3 ~= 0.67 which results in the following allocation:

1to3 -> 0.67 Mbps
1to2 -> 1.33 Mbps
2to3 -> 1.33 Mbps

We see that unlike with max-min fairness, proportional fairness does penalize 1to3 for using more links in the network.

[Q27]
Assuming no queuing delays, the RTT for each of these flows should be 300ms (150ms delay for each way).

[Q28]
The measured goodputs are:

[Q28.a] 1to2 -> ~1110 kbps
[Q28.b] 1to3 ->  ~740 kbps
[Q28.c] 2to3 -> ~1110 kbps

[Q29]
We could say that this corresponds more or less to our analysis, since we know that CUBIC performs similarly to RENO for relatively small RTTs, which has a utility function in between max-min fairness and proportional fairness, and the measured goodputs are kind of in between the allocations we found with max-min fairness and proportional fairness.

[Q30]
The average RTT of all three connections is around 400ms. Since we added 300ms of delay to the RTT on all flows, we still have approximately 100ms of queuing delay. Assuming also that the two routers are facing equal queuing delays, this means that the queuing delay on each of these routers is around 50ms.

[Q31]
Here are the measured goodputs with TCP CUBIC:

1to2 -> ~1100kbps
1to3 ->  ~670kbps
2to3 -> ~1120kbps

We can see that the goodput of 1to3 is significantly smaller than those of 1to2 and 2to3. The results didn't really change that much from when the delay at h2 was 150ms. This proves that CUBIC is independent of the RTTs of the flows.

[Q32]
Here are the measured goodputs with TCP RENO:

1to2 ->  ~420kbps
1to3 -> ~1310kbps
2to3 ->  ~420kbps

We can see that this time, the goodput of 1to3 is significantly larger than those of 1to2 and 2to3. The results here are kind of the opposite of what we obtained with TCP CUBIC. This shows again that RENO depends highly on the RTTs of the flows. In contrast, CUBIC does not and therefore works better than RENO in a regime where the RTT or product delay-bandwidth are small. For instance, we see that in this last scenario where we increased the outgoing delay of h2's interface, CUBIC handled the situation better and kept the rates of 1to2 and 2to3 higher than 1to3.

[Q33]
Let n1 be the first node connecting A and C to the common link,
Let n2 be the node connecting B and D to the common link.
At n1, the ratio of accepted traffic is 2.5/20+2 = 0.1136
Therefore on the common link, A's flow is at max 0.2272=2 * 0.1136 Mbps and C's
flow at max 2.272 Mbps.
At n2, A's flow can go to B without rate reduction, while C's flow has to be red
uced to at max 0.5 Mbps.

Finally, the max goodput at B is 0.2272 Mbps and the max goodput at D is 0.5 Mbp
s.

[Q34]
Applying the water filling algorithm, we first set xA and xC to 0.5Mbps, and the
n we can improve xA to be 2 Mbps.
Then no improvments can be made because it saturates the common link.
Therefore a max-min allocation is 2Mbps for A, 0.5Mbps for C.

[Q35]
We have this set of constraints for A's rate xA and C's rate xC :
xA + xC <= 2.5
xC <= min(0.5, 20) = 0.5
xA <= min(2, 20) = 2
But xA, xC = 2, 0.5 satisfies equality on theses constraints, therefore is a proportional allocation.

[Q36]
- h1 <-> h4
goodput :56.4 (varies a lot from 2kbps to 480kbps)
loss : 93.331%
- h2 <-> h3
goodput : 480 kbps
loss: 97.614%

[Q37]
With rates 2Mbps for h1 and 0.5Mbps for h2, we get 
- h1 <-> h4
goodput : 1911.1 kbps
loss : 4.516%
- h2 <-> h3
goodput : 479.9 kbps
loss: 3.460%

We clearly see an improvements in the loss rate, even though the rates are lower than in q36 : congestion collapse.

[Q38] 
- h1 <-> h4 goodput : 1849.1kbps
- h2 <-> h3 goodput : 467.4kbps
Which is close to the max min allocation (a bit worse).

[Q39]
We saw that congestion collapse occures when the sending rates are decided independently of the network topology and with no congestion control.
With knowledge of the network topology and bottlenecks, one can set the rates so that no congestion control is necessary.
However a stable network topology is rare, therefore the best way to ensure no congestion collapse and use the links fairly and efficiently is to have an adaptative congestion control mechanism, which have advantages :
- automatic, no knowledge required about the current network states.
- adaptative, adapts itself to the current network state
- avoids congestion
- divide "fairly" the network ressources between flows
- allows a good efficiency for links and good usage

